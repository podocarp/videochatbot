{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    TextIteratorStreamer,\n",
    "    pipeline,\n",
    ")\n",
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from threading import Thread\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "from IPython.display import Audio, Video\n",
    "from Wav2Lip import inference as wav2lip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c3ad8c235442ad8da99be2cb3b8d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LLM model.\n",
      "Loaded text to speech model.\n"
     ]
    }
   ],
   "source": [
    "# Load the models and stuff\n",
    "# IMPORTANT: If you want to rerun this cell, restart the kernel! (or delete the model variable)\n",
    "# The old model is still eating VRAM, transformers may start loading into CPU RAM!\n",
    "# Should not run for more than 30s!\n",
    "\n",
    "modelPath = \"/home/models/vicuna/13B-1.3\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(modelPath)\n",
    "llmModel = AutoModelForCausalLM.from_pretrained(modelPath, device_map=\"auto\", load_in_8bit=True, torch_dtype=torch.float16)\n",
    "print(\"Loaded LLM model.\")\n",
    "\n",
    "speechModelPath = \"/home/models/speecht5_tts/\"\n",
    "ganPath = \"/home/models/speecht5_hifigan/\"\n",
    "processor = SpeechT5Processor.from_pretrained(speechModelPath)\n",
    "speechModel = SpeechT5ForTextToSpeech.from_pretrained(speechModelPath)\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(ganPath)\n",
    "print(\"Loaded text to speech model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmPipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=llmModel,\n",
    "    temperature=0,\n",
    "    max_new_tokens=512,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. The AI is helpful, detailed, but also concise.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "llm = HuggingFacePipeline(pipeline=llmPipeline)\n",
    "llmChain = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=False,\n",
    "    memory=ConversationSummaryBufferMemory(llm=llm, k=3, ai_prefix=\"ASSISTANT\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_embeddings = {\n",
    "    \"BDL\": \"spkemb/cmu_us_bdl_arctic-wav-arctic_a0009.npy\", # male\n",
    "    \"CLB\": \"spkemb/cmu_us_clb_arctic-wav-arctic_a0144.npy\", # female\n",
    "    \"KSP\": \"spkemb/cmu_us_ksp_arctic-wav-arctic_b0087.npy\", # male\n",
    "    \"RMS\": \"spkemb/cmu_us_rms_arctic-wav-arctic_b0353.npy\", # male\n",
    "    \"SLT\": \"spkemb/cmu_us_slt_arctic-wav-arctic_a0508.npy\", # female\n",
    "    }\n",
    "\n",
    "def __textToSpeech(text, speaker):\n",
    "    if len(text.strip()) == 0:\n",
    "        return np.zeros(0).astype(np.int16)\n",
    "\n",
    "    inputs = processor(text=text, return_tensors=\"pt\")\n",
    "\n",
    "    # limit input length\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    input_ids = input_ids[..., :speechModel.config.max_text_positions]\n",
    "    speaker_embedding = np.load(speaker_embeddings[speaker[:3]])\n",
    "    speaker_embedding = torch.tensor(speaker_embedding).unsqueeze(0)\n",
    "    speech = speechModel.generate_speech(input_ids, speaker_embedding, vocoder=vocoder)\n",
    "    speech = (speech.numpy() * 32767).astype(np.int16)\n",
    "    \n",
    "    return speech\n",
    "\n",
    "def textToSpeech(text:str, speaker=\"RMS\"):\n",
    "    sentences = text.split(\"\\n\")\n",
    "    outputs = []\n",
    "    for sentence in sentences:\n",
    "        outputs.append(__textToSpeech(sentence, speaker))\n",
    "\n",
    "    return np.concatenate(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading video frames...\n",
      "Number of frames available for inference: 2398\n",
      "(80, 3759)\n",
      "Length of mel chunks: 1404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:25<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoint from: checkpoints/wav2lip_gan.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/44 [00:26<19:12, 26.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:30<00:00,  1.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"result.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Wav2Lip import inference as wav2lip\n",
    "wav2lip.main(video_path=\"videos/kennedy1min.mp4\", \n",
    "        audio_path=\"temp/tts.wav\",\n",
    "        out_path=\"result.mp4\", \n",
    "        checkpoint_path=\"checkpoints/wav2lip_gan.pth\",\n",
    "        batch_size=32, is_static=False)\n",
    "Video(\"result.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryLLMWithVoice(input):\n",
    "    output = llmChain.predict(input=input)\n",
    "    print(output)\n",
    "    speech = textToSpeech(output)\n",
    "    write('temp/tts.wav', 16000, speech)\n",
    "    return Audio(speech, rate=16000)\n",
    "\n",
    "def queryLLMWithVideo(input):\n",
    "    text = llmChain.predict(input=input)\n",
    "    print(text)\n",
    "    speech = textToSpeech(text)\n",
    "    write('temp/tts.wav', 16000, speech)\n",
    "    wav2lip.main(video_path=\"videos/kennedy1min.mp4\", \n",
    "        audio_path=\"temp/tts.wav\",\n",
    "        out_path=\"result.mp4\", batch_size=32, static=False)\n",
    "    return Video(\"result.mp4\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  As President John F. Kennedy, I do not believe it is appropriate for me to comment on the personal characteristics of individual politicians. It is important for me to focus on the issues and policies that are important to our country and to work towards finding constructive solutions to the challenges we face.\n",
      "\n",
      "I believe that it is important for our country to come together and to work towards finding common ground, regardless of our political differences. I also believe in the importance of upholding the values and principles that have made our country great, including the rule of law, respect for human rights, and a commitment to democracy and freedom.\n",
      "\n",
      "Overall, I believe that it is important for us to focus on the issues that matter and to work towards finding constructive solutions to the challenges we face as a nation.\n",
      "Reading video frames...\n",
      "Number of frames available for inference: 2398\n",
      "(80, 3759)\n",
      "Length of mel chunks: 1404\n",
      "Load checkpoint from: checkpoints/wav2lip_gan.pth\n",
      "model loaded\n",
      "generating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/44 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning face detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:32<00:00,  1.36it/s]\n",
      "100%|██████████| 44/44 [00:37<00:00,  1.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"result.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryLLMWithVideo(\"You are Kennedy. What do you think of Donald Trump as a person?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
